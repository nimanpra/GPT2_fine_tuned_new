# -*- coding: utf-8 -*-
"""GRADIO_GPT2_Text_Generation(The_Power_Of_Now).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n9jMZEDpedjTFupco45HkAlZVCsuTApG

## Using Gradio to wrap a text to text interface around GPT-2 

Check out the library on [github](https://github.com/gradio-app/gradio-UI) and see the [getting started](https://gradio.app/getting_started.html) page for more demos.

### Installs and Imports
"""
huggingface_user_name = "nimanpra"
huggingface_api_key = "api_pDPmTOWqnAMaDypwsRbRsdHscZLlANIiTj"

# Commented out IPython magic to ensure Python compatibility.
import torch
import gradio as gr
from transformers import GPT2Tokenizer, GPT2LMHeadModel

"""### Loading the model and creating the generate function"""

available_models = {}

model_from_huggingface = GPT2LMHeadModel.from_pretrained(
    f"{huggingface_user_name}/Fine_Tuned_Spiritual", use_auth_token=huggingface_api_key
)
tokenizer = GPT2Tokenizer.from_pretrained(
    f"{huggingface_user_name}/Fine_Tuned_Spiritual", use_auth_token=huggingface_api_key
)

if torch.cuda.is_available():
    torch.cuda.empty_cache()
    model_from_huggingface.cuda()
else:
    device = torch.device("cpu")
    model_from_huggingface.to(device)

available_models["Model_Spiritual"] = [model_from_huggingface, tokenizer]


def generate_text_from_saved_model(genre, topic):
    model_from_huggingface = available_models[f"Model_{genre}"][0]
    tokenizer = available_models[f"Model_{genre}"][1]

    model_from_huggingface.eval()
    with torch.no_grad():
        gpt2_generated_text = {}
        prompt = f"<|startoftext|> {topic}"

        generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
        generated = generated.to(device)

        sample_outputs = model_from_huggingface.generate(
            generated,
            do_sample=True,
            top_k=50,
            min_length=768,
            max_length=1024,
            top_p=0.95,
            num_return_sequences=1,
        )
        print("got through")
        for i, sample_output in enumerate(sample_outputs):
            print(
                f"{i}: {tokenizer.decode(sample_output, skip_special_tokens=True)}\n\n"
            )
            gpt2_generated_text[f"Gen_Text_{i+1}"] = tokenizer.decode(
                sample_output, skip_special_tokens=True
            )
    torch.cuda.empty_cache()
    keys = list(gpt2_generated_text.keys())
    return gpt2_generated_text[keys[0]]


"""###Creating the interface and launching!"""

output_text = gr.outputs.Textbox()
io = gr.Interface(
    fn=generate_text_from_saved_model,
    inputs=[
        gr.inputs.Radio(["Spiritual", "Self-help", "Biohacks", "Nutrition"]),
        gr.inputs.Textbox(lines=2, placeholder="Enter your topic..."),
    ],
    outputs="text",
    title="GPT-2 Genre-Wise Content Generator",
    description="We have fine tuned the model with given Corpus. At the moment the training was done only for the Spiritual Genre",
)
io.launch(debug=True, share=True)
